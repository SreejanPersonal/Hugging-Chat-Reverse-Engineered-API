import requests
import json

class HuggingChat_RE:
    def __init__(self, hf_chat: str, model: str = "meta-llama/Meta-Llama-3-70B-Instruct") -> None:
        """
        Initializes an instance of the HuggingChat_RE class.

        Parameters:
        - hf_chat (str): The Hugging Face chat token.
        - model (str): The name or path of the model to be used for the chat. Defaults to "meta-llama/Meta-Llama-3-70B-Instruct".

        Returns:
        - None: This is a constructor method and does not return anything.
        """

        self.hf_chat = hf_chat
        self.model = model
        self.headers = {
            "Cookie": f"hf-chat={self.hf_chat}",
        }
        self.conversationId = self.find_conversation_id()
        self.messageId = self.find_message_id()

    def find_conversation_id(self) -> str:
        """
        Finds and returns the conversation ID for the Hugging Face chat.

        Returns:
        - str: The conversation ID retrieved from the server response.
        """

        url = "https://huggingface.co/chat/conversation"
        payload = {"model": self.model}
        response = requests.post(url, json=payload, headers=self.headers).json()
        print("\033[92m" + "Initialised Conversation ID:", response['conversationId'] + "\033[0m")
        return response['conversationId']

    def find_message_id(self) -> str:
        """
        Finds and returns the message ID for the Hugging Face chat.

        Returns:
        - str: The message ID retrieved from the server response.
        """

        url = f"https://huggingface.co/chat/conversation/{self.conversationId}/__data.json?x-sveltekit-invalidated=11"
        response = requests.get(url, headers=self.headers).json()
        print("\033[92m" + "Initialised Message ID:", response['nodes'][1]['data'][3] + "\033[0m")
        return response['nodes'][1]['data'][3]

    def generate(self, query: str, web_search: bool = False, files=[], stream: bool = True) -> str:
        """
        Generates a response for the given query using the Hugging Face chat.

        Parameters:
        - query (str): The text query to generate a response for.
        - web_search (bool): A flag indicating whether to perform web search in the response generation process. Defaults to False.
        - files (List[str]): A list of file paths to include in the query. Defaults to an empty list.
        - stream (bool): A flag indicating whether to stream the response. Defaults to True.

        Returns:
        - str: The complete response generated by the chat.
        """

        url = f"https://huggingface.co/chat/conversation/{self.conversationId}"
        payload = {
            "inputs": query,
            "id": self.messageId,
            "is_retry": False,
            "is_continue": False,
            "web_search": web_search,
            "files": files
        }


        response = requests.post(url, json=payload, headers=self.headers, stream=True)
        complete_response = ""
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                try:
                    json_data = json.loads(chunk.decode("utf-8"))
                    if json_data['type'] == "stream":
                        if stream: print(json_data['token'], end="", flush=True)
                        complete_response += json_data['token']
                except:
                    continue

        return complete_response



# Example Usage
if __name__ == "__main__":
    hf_api = HuggingChat_RE(model="microsoft/Phi-3-mini-4k-instruct")
    while True:
        query = input("\n> ")
        response = hf_api.generate(query, web_search=False)
        # response = hf_api.generate(query, stream=False)
        # print(response)